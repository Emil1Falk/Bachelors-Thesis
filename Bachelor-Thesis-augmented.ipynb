{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ca719f3f-8019-4072-a589-9ce4a8726818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import unicodedata\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import shap\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras import models, layers\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import os\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.single_table import CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0155b739-58ba-438a-8612-1c1fa2c57b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('/Users/emilfalk/Desktop/X_train.csv')\n",
    "X_test = pd.read_csv('/Users/emilfalk/Desktop/X_test.csv')\n",
    "y_train = pd.read_csv('/Users/emilfalk/Desktop/y_train.csv')\n",
    "y_test = pd.read_csv('/Users/emilfalk/Desktop/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e4a23e1-5e01-4bd1-89ec-679d0983ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['Player']) \n",
    "X_test = X_test.drop(columns=['Player']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "33932134-9d6e-4295-b4b4-88b42f513788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['years_with_team', 'draft_pos', 'all_star', 'num_teams', 'ig_followers', 'resigned', 'Agent'], axis=1)\n",
    "X_test = X_test.drop(['years_with_team', 'draft_pos', 'all_star', 'num_teams', 'ig_followers', 'resigned', 'Agent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ff7b0854-4bc6-4d6c-867b-f5f59fdd03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sdv/single_table/base.py:123: UserWarning:\n",
      "\n",
      "We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\\n    X_aug, y_aug, test_size=0.2, random_state=42\\n)'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "df_train = X_train.copy()\n",
    "df_train['salary'] = y_train.values.ravel() \n",
    "\n",
    "metadata = Metadata.detect_from_dataframe(data=df_train)\n",
    "\n",
    "synthesizer = CTGANSynthesizer(metadata, epochs=4500)\n",
    "synthesizer.fit(df_train)\n",
    "\n",
    "synthetic_data = synthesizer.sample(num_rows=7000)\n",
    "\n",
    "X_synth = synthetic_data.drop(columns=['salary'])\n",
    "y_synth = synthetic_data[['salary']].copy()  \n",
    "y_synth.columns = ['Salary']   \n",
    "\n",
    "\n",
    "X_aug = pd.concat([X_train, X_synth], ignore_index=True)\n",
    "y_aug = pd.concat([y_train, y_synth], ignore_index=True)\n",
    "\n",
    "\"\"\"X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_aug, y_aug, test_size=0.2, random_state=42\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "64533cbe-f19d-4a14-a3f5-2cd782e6fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_aug) \n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ee423f6f-2990-427d-816a-284d84732b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45341b6-f9e6-4757-b3b1-9eb1eed7ea28",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6c59a82d-c6b0-4272-b1c6-004b1d9ba90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge alpha (Basketball Features): 100.0\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(random_state=42)\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "grid_search_basketball = GridSearchCV(ridge, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best Ridge alpha (Basketball Features): {grid_search_basketball.best_params_['alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e0349-2b83-4164-85fb-8fd3be2e54dc",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8a105afb-acfa-4aca-9fdb-9c9f10e2fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso alpha (Basketball Features): 100.0\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(random_state=42)\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "\n",
    "grid_search_basketball = GridSearchCV(lasso, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best Lasso alpha (Basketball Features): {grid_search_basketball.best_params_['alpha']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccfabba-d35c-4cc4-976f-ede76db77183",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "16a850b2-7722-40df-8d19-e87e1f689de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree params (Basketball Features): {'max_depth': 5, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_basketball = GridSearchCV(dt, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best Decision Tree params (Basketball Features): {grid_search_basketball.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd841e9a-43c0-4356-8e6d-855eca8a1e72",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "30650df2-cefe-420b-80fe-e04967e70773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest params (Basketball Features): {'max_depth': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "grid_search_basketball = GridSearchCV(rf, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best Random Forest params (Basketball Features): {grid_search_basketball.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4a086-8133-4620-aa12-0a5fea951078",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9623ad08-03fa-4762-99b0-2269fa7b5617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params (Basketball Features): {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search_basketball = GridSearchCV(xgb, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best XGBoost params (Basketball Features): {grid_search_basketball.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6917f7-7105-4bb7-892b-1acb0e3f33a4",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e5786899-3a60-4fc6-b887-36ef4ad0acae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MLP params (Basketball Features): {'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(random_state=42, max_iter=300)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_basketball = GridSearchCV(mlp, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search_basketball.fit(X_train_scaled, y_aug.values.ravel())\n",
    "print(f\"Best MLP params (Basketball Features): {grid_search_basketball.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ce5bffe7-a495-4b0f-a56c-76663bb78b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnostics for Linear Regression:\n",
      "Basketball Features MAE Folds: ['4530240.20', '4632500.82', '4460727.12', '4636020.62', '4740389.54']\n",
      "Basketball Features MAE Mean: 4599975.66\n",
      "Basketball Features MAE Std: 107617.81\n",
      "Basketball Features MAE: 4310212.3404\n",
      "Basketball Features R2: 0.6393\n",
      "\n",
      "Diagnostics for Ridge Regression:\n",
      "Basketball Features MAE Folds: ['4536347.27', '4635419.32', '4465140.11', '4645482.42', '4741717.48']\n",
      "Basketball Features MAE Mean: 4604821.32\n",
      "Basketball Features MAE Std: 106688.40\n",
      "Basketball Features MAE: 4305512.5470\n",
      "Basketball Features R2: 0.6407\n",
      "\n",
      "Diagnostics for Lasso Regression:\n",
      "Basketball Features MAE Folds: ['4530256.80', '4632473.89', '4460708.44', '4636057.96', '4740371.42']\n",
      "Basketball Features MAE Mean: 4599973.70\n",
      "Basketball Features MAE Std: 107616.35\n",
      "Basketball Features MAE: 4310204.7896\n",
      "Basketball Features R2: 0.6393\n",
      "\n",
      "Diagnostics for Decision Tree:\n",
      "Basketball Features MAE Folds: ['4357182.56', '4402120.55', '4201973.23', '4335299.72', '4530140.54']\n",
      "Basketball Features MAE Mean: 4365343.32\n",
      "Basketball Features MAE Std: 118500.73\n",
      "Basketball Features MAE: 4220120.5132\n",
      "Basketball Features R2: 0.5227\n",
      "\n",
      "Diagnostics for Random Forest:\n",
      "Basketball Features MAE Folds: ['4077504.38', '4164834.15', '3841003.40', '4139368.53', '4327775.01']\n",
      "Basketball Features MAE Mean: 4110097.10\n",
      "Basketball Features MAE Std: 176571.76\n",
      "Basketball Features MAE: 4177958.5533\n",
      "Basketball Features R2: 0.5702\n",
      "\n",
      "Diagnostics for XGBoost:\n",
      "Basketball Features MAE Folds: ['4056040.25', '4141405.50', '3803867.75', '4067487.50', '4219496.50']\n",
      "Basketball Features MAE Mean: 4057659.50\n",
      "Basketball Features MAE Std: 156296.25\n",
      "Basketball Features MAE: 4159098.0000\n",
      "Basketball Features R2: 0.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnostics for MLP:\n",
      "Basketball Features MAE Folds: ['4050907.90', '4219692.76', '3926304.70', '4113868.59', '4212231.95']\n",
      "Basketball Features MAE Mean: 4104601.18\n",
      "Basketball Features MAE Std: 122054.23\n",
      "Basketball Features MAE: 3978935.4695\n",
      "Basketball Features R2: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    {\n",
    "        'name': 'Linear Regression',\n",
    "        'model_basketball': LinearRegression()\n",
    "    },\n",
    "    {\n",
    "        'name': 'Ridge Regression',\n",
    "        'model_basketball': Ridge(alpha=100.0)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Lasso Regression',\n",
    "        'model_basketball': Lasso(alpha=100.0)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model_basketball': DecisionTreeRegressor(max_depth=5, min_samples_split=2)\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model_basketball': RandomForestRegressor(max_depth=10, n_estimators=200)\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'model_basketball': XGBRegressor(learning_rate=0.1, max_depth=3, n_estimators=100)\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLP',\n",
    "        'model_basketball': MLPRegressor(hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300)\n",
    "    }\n",
    "]\n",
    "\n",
    "cv_folds = 5\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "mae_scale = 1_000_000\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, feature_set, model_name):\n",
    "    if X_train.ndim > 2:\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    if X_test.ndim > 2:\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mae_folds = []\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_train_fold = X_train[train_idx]\n",
    "        X_val_fold = X_train[val_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx] if isinstance(y_train, pd.DataFrame) else y_train[train_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx] if isinstance(y_train, pd.DataFrame) else y_train[val_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold.values.ravel() if isinstance(y_train_fold, pd.DataFrame) else y_train_fold)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        mae_folds.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    cv_mae_std = np.std(mae_folds, ddof=1) / mae_scale\n",
    "    \n",
    "    return model, mae_folds, y_test_pred, test_mae, test_r2\n",
    "\n",
    "for model_info in models:\n",
    "    name = model_info['name']\n",
    "    model_basketball = model_info['model_basketball']\n",
    "    \n",
    "    _, mae_basketball_folds, y_test_pred_basketball, test_mae, test_r2 = evaluate_model(\n",
    "        model_basketball, \n",
    "        X_train_scaled, \n",
    "        X_test_scaled, \n",
    "        y_aug, \n",
    "        y_test, \n",
    "        'Basketball Features', \n",
    "        name\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDiagnostics for {name}:\")\n",
    "    print(f\"Basketball Features MAE Folds: {[f'{mae:.2f}' for mae in mae_basketball_folds]}\")\n",
    "    print(f\"Basketball Features MAE Mean: {np.mean(mae_basketball_folds):.2f}\")\n",
    "    print(f\"Basketball Features MAE Std: {np.std(mae_basketball_folds, ddof=1):.2f}\")\n",
    "    print(f\"Basketball Features MAE: {test_mae:.4f}\")\n",
    "    print(f\"Basketball Features R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2aa267c0-4c89-4aaf-a241-d4c9a09b222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in randomized search (Basketball Features): 'super' object has no attribute '__sklearn_tags__'\n",
      "\n",
      "1D CNN Regression Results:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "import pandas as pd\n",
    "import time\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_cnn_model(learning_rate=0.001, l2_reg=0.01, dropout_rate_1=0.2, \n",
    "                     dropout_rate_2=0.2, dropout_rate_3=0.3, filters_1=32, \n",
    "                     filters_2=16, dense_units=50, input_dim=None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=filters_1, kernel_size=3, activation='relu', \n",
    "                              input_shape=(input_dim, 1), padding='same'),\n",
    "        tf.keras.layers.Dropout(dropout_rate_1),\n",
    "        tf.keras.layers.Conv1D(filters=filters_2, kernel_size=3, activation='relu', \n",
    "                              padding='same'),\n",
    "        tf.keras.layers.Dropout(dropout_rate_2),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu', \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(dropout_rate_3),\n",
    "        tf.keras.layers.Dense(1)  \n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, feature_set):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        mae_folds = []\n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_train_fold = X_train[train_idx]\n",
    "            X_val_fold = X_train[val_idx]\n",
    "            y_train_fold = y_train.iloc[train_idx] if isinstance(y_train, pd.DataFrame) else y_train[train_idx]\n",
    "            y_val_fold = y_train.iloc[val_idx] if isinstance(y_train, pd.DataFrame) else y_train[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, verbose=0)\n",
    "            y_pred_fold = model.predict(X_val_fold, verbose=0).flatten()\n",
    "            mae_folds.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "        \n",
    "        cv_mae_mean = np.mean(mae_folds)\n",
    "        cv_mae_std = np.std(mae_folds, ddof=1)\n",
    "        \n",
    "        print(f\"\\nDiagnostics for 1D CNN ({feature_set}):\")\n",
    "        print(f\"CV MAE Folds: {[f'{mae:.2f}' for mae in mae_folds]}\")\n",
    "        print(f\"CV MAE Mean: {cv_mae_mean:.2f}\")\n",
    "        print(f\"CV MAE Std: {cv_mae_std:.2f}\")\n",
    "        print(f\"Training Time (s): {train_time:.4f}\")\n",
    "\n",
    "        mae_scale = 1_000_000\n",
    "        results.append({\n",
    "            'Model': '1D CNN Regression',\n",
    "            'Feature Set': feature_set,\n",
    "            'Dataset': 'Test',\n",
    "            'MAE': test_mae,\n",
    "            'R2': test_r2,\n",
    "            'CV MAE Mean': cv_mae_mean,\n",
    "            'CV MAE Std': cv_mae_std\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for {feature_set}: {e}\")\n",
    "        results.append({\n",
    "            'Model': '1D CNN Regression',\n",
    "            'Feature Set': feature_set,\n",
    "            'Dataset': 'Test',\n",
    "            'MAE': np.nan,\n",
    "            'R2': np.nan,\n",
    "            'CV MAE Mean': np.nan,\n",
    "            'CV MAE Std': np.nan\n",
    "        })\n",
    "    return model\n",
    "\n",
    "param_dist = {\n",
    "    'model__learning_rate': uniform(1e-5, 1e-3),  \n",
    "    'model__l2_reg': uniform(0.01, 0.1),         \n",
    "    'model__dropout_rate_1': uniform(0.2, 0.4),  \n",
    "    'model__dropout_rate_2': uniform(0.2, 0.4), \n",
    "    'model__dropout_rate_3': uniform(0.3, 0.4),  \n",
    "    'model__filters_1': randint(16, 64),        \n",
    "    'model__filters_2': randint(8, 32),         \n",
    "    'model__dense_units': randint(20, 100),     \n",
    "    'batch_size': [8, 16, 32]\n",
    "}\n",
    "\n",
    "try:\n",
    "    cnn_basketball = KerasRegressor(\n",
    "        model=create_cnn_model,\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "        input_dim=X_train_scaled_cnn.shape[1]\n",
    "    )\n",
    "    grid_search_basketball = RandomizedSearchCV(cnn_basketball, param_dist, n_iter=10, cv=5, \n",
    "                                               scoring='r2', n_jobs=1, random_state=42, \n",
    "                                               error_score='raise')\n",
    "    grid_search_basketball.fit(X_train_scaled_cnn, y_aug.values.ravel())\n",
    "    cnn_basketball = evaluate_model(grid_search_basketball.best_estimator_, \n",
    "                                   X_train_scaled_cnn, X_test_scaled_cnn, \n",
    "                                   y_aug, y_test, 'Basketball Features')\n",
    "    print(f\"Best 1D CNN params (Basketball Features): {grid_search_basketball.best_params_}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in randomized search (Basketball Features): {e}\")\n",
    "    cnn_basketball = None\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  \n",
    "\n",
    "print(\"\\n1D CNN Regression Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff51d9fb-78a6-43e3-8aa7-1ea62718b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 50 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3988bd440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnostics for 1D CNN (Basketball Features):\n",
      "CV MAE Folds: ['7177881.00', '6871311.00', '7142454.50', '7285862.50', '7768103.00']\n",
      "CV MAE Mean: 7249122.40\n",
      "CV MAE Std: 327828.28\n",
      "Training Time (s): 21.0219\n",
      "\n",
      "1D CNN Regression Results:\n",
      "               Model          Feature Set Dataset          MAE     R2  \\\n",
      "0  1D CNN Regression  Basketball Features    Test 6030550.5000 0.0225   \n",
      "\n",
      "   CV MAE Mean  CV MAE Std  \n",
      "0 7249122.4000 327828.2821  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import time\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the CNN model creation function\n",
    "def create_cnn_model(learning_rate=0.001, l2_reg=0.01, dropout_rate_1=0.44, \n",
    "                     dropout_rate_2=0.2, dropout_rate_3=0.3, filters_1=42, \n",
    "                     filters_2=17, dense_units=95, input_dim=None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=filters_1, kernel_size=3, activation='relu', \n",
    "                              input_shape=(input_dim, 1), padding='same'),\n",
    "        tf.keras.layers.Dropout(dropout_rate_1),\n",
    "        tf.keras.layers.Conv1D(filters=filters_2, kernel_size=3, activation='relu', \n",
    "                              padding='same'),\n",
    "        tf.keras.layers.Dropout(dropout_rate_2),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu', \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        tf.keras.layers.Dropout(dropout_rate_3),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, feature_set):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train.values.ravel(), epochs=50, batch_size=8, verbose=0)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        mae_folds = []\n",
    "        for train_idx, val_idx in cv.split(X_train):\n",
    "            X_train_fold = X_train[train_idx]\n",
    "            X_val_fold = X_train[val_idx]\n",
    "            y_train_fold = y_train.iloc[train_idx] if isinstance(y_train, pd.DataFrame) else y_train[train_idx]\n",
    "            y_val_fold = y_train.iloc[val_idx] if isinstance(y_train, pd.DataFrame) else y_train[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=32, verbose=0)\n",
    "            y_pred_fold = model.predict(X_val_fold, verbose=0).flatten()\n",
    "            mae_folds.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "        \n",
    "        cv_mae_mean = np.mean(mae_folds)\n",
    "        cv_mae_std = np.std(mae_folds, ddof=1)\n",
    "        \n",
    "        print(f\"\\nDiagnostics for 1D CNN ({feature_set}):\")\n",
    "        print(f\"CV MAE Folds: {[f'{mae:.2f}' for mae in mae_folds]}\")\n",
    "        print(f\"CV MAE Mean: {cv_mae_mean:.2f}\")\n",
    "        print(f\"CV MAE Std: {cv_mae_std:.2f}\")\n",
    "        print(f\"Training Time (s): {train_time:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            'Model': '1D CNN Regression',\n",
    "            'Feature Set': feature_set,\n",
    "            'Dataset': 'Test',\n",
    "            'MAE': test_mae,\n",
    "            'R2': test_r2,\n",
    "            'CV MAE Mean': cv_mae_mean,\n",
    "            'CV MAE Std': cv_mae_std\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model for {feature_set}: {e}\")\n",
    "        results.append({\n",
    "            'Model': '1D CNN Regression',\n",
    "            'Feature Set': feature_set,\n",
    "            'Dataset': 'Test',\n",
    "            'MAE': np.nan,\n",
    "            'R2': np.nan,\n",
    "            'CV MAE Mean': np.nan,\n",
    "            'CV MAE Std': np.nan\n",
    "        })\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model without RandomizedSearchCV\n",
    "try:\n",
    "    # Create KerasRegressor instance\n",
    "    cnn_basketball = KerasRegressor(\n",
    "        model=create_cnn_model,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        input_dim=X_train_scaled_cnn.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model directly\n",
    "    cnn_basketball = evaluate_model(\n",
    "        cnn_basketball,\n",
    "        X_train_scaled_cnn,\n",
    "        X_test_scaled_cnn,\n",
    "        y_aug,\n",
    "        y_test,\n",
    "        'Basketball Features'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error training model (Basketball Features): {e}\")\n",
    "    cnn_basketball = None\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(\"\\n1D CNN Regression Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fd32ebe3-86cf-4f9c-8af4-9bf9c27efbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_1d_cnn(input_dim = 28, output_dim=1, learning_rate=0.01, \n",
    "                            filters_1=16, filters_2=8, dense_units=50, \n",
    "                            dropout_rate_1=0.3, dropout_rate_2=0.3, dropout_rate_3=0.4):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Fully connected layer to increase dimension\n",
    "    model.add(layers.Dense(128, input_shape=(input_dim,), activation='relu'))\n",
    "    \n",
    "    # Reshape to (16, 8) for Conv1D input\n",
    "    model.add(layers.Reshape((16, 8)))\n",
    "    \n",
    "    # First Conv1D layer with batch normalization and dropout\n",
    "    model.add(layers.Conv1D(filters=filters_1, kernel_size=3, activation='leaky_relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate_1))\n",
    "    \n",
    "    # Second Conv1D layer with batch normalization and dropout\n",
    "    model.add(layers.Conv1D(filters=filters_2, kernel_size=3, activation='leaky_relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate_2))\n",
    "    \n",
    "    # Flatten the output\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Dense layer for regression\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate_3))\n",
    "    \n",
    "    # Output layer for regression\n",
    "    model.add(layers.Dense(output_dim))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d7fd09e2-137a-449d-b657-65e225035917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step - loss: 194228822999040.0000 - val_loss: 153982915837952.0000\n",
      "Epoch 2/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 143531062591488.0000 - val_loss: 56392786378752.0000\n",
      "Epoch 3/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 61652527480832.0000 - val_loss: 42179208675328.0000\n",
      "Epoch 4/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 49758404083712.0000 - val_loss: 41456865640448.0000\n",
      "Epoch 5/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 46674428821504.0000 - val_loss: 40496659431424.0000\n",
      "Epoch 6/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 46801935663104.0000 - val_loss: 40307706036224.0000\n",
      "Epoch 7/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 44698722893824.0000 - val_loss: 40426677469184.0000\n",
      "Epoch 8/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 43743675678720.0000 - val_loss: 40006097829888.0000\n",
      "Epoch 9/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 45133240205312.0000 - val_loss: 39773997629440.0000\n",
      "Epoch 10/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 43576176148480.0000 - val_loss: 40143553560576.0000\n",
      "Epoch 11/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 42975287574528.0000 - val_loss: 39917535100928.0000\n",
      "Epoch 12/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 41653167456256.0000 - val_loss: 40008975122432.0000\n",
      "Epoch 13/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 42528254459904.0000 - val_loss: 39876414144512.0000\n",
      "Epoch 14/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 41744670392320.0000 - val_loss: 39828045430784.0000\n",
      "Epoch 15/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 40314328842240.0000 - val_loss: 40001781891072.0000\n",
      "Epoch 16/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 39830461349888.0000 - val_loss: 39987479314432.0000\n",
      "Epoch 17/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 40529987371008.0000 - val_loss: 40219118141440.0000\n",
      "Epoch 18/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 40327385710592.0000 - val_loss: 40327880638464.0000\n",
      "Epoch 19/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 39415707598848.0000 - val_loss: 40706810839040.0000\n",
      "Epoch 20/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 38933069037568.0000 - val_loss: 41047568678912.0000\n",
      "Epoch 21/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 38108762472448.0000 - val_loss: 40928479805440.0000\n",
      "Epoch 22/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 37399853793280.0000 - val_loss: 40943357001728.0000\n",
      "Epoch 23/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 37423459336192.0000 - val_loss: 41490013224960.0000\n",
      "Epoch 24/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 37716347584512.0000 - val_loss: 41723711455232.0000\n",
      "Epoch 25/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 36227638099968.0000 - val_loss: 42504548253696.0000\n",
      "Epoch 26/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 36706497593344.0000 - val_loss: 41799246675968.0000\n",
      "Epoch 27/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 37045581905920.0000 - val_loss: 42444414517248.0000\n",
      "Epoch 28/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 35820513787904.0000 - val_loss: 42653626400768.0000\n",
      "Epoch 29/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 34399963840512.0000 - val_loss: 43144418689024.0000\n",
      "Epoch 30/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 35224691933184.0000 - val_loss: 43535017443328.0000\n",
      "Epoch 31/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 34114359001088.0000 - val_loss: 43797207580672.0000\n",
      "Epoch 32/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 33515443847168.0000 - val_loss: 43448199544832.0000\n",
      "Epoch 33/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 34581252145152.0000 - val_loss: 43000738611200.0000\n",
      "Epoch 34/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 33317644664832.0000 - val_loss: 43963276853248.0000\n",
      "Epoch 35/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 33052076015616.0000 - val_loss: 44139685085184.0000\n",
      "Epoch 36/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 34618711474176.0000 - val_loss: 44155875098624.0000\n",
      "Epoch 37/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 33998371815424.0000 - val_loss: 44260057415680.0000\n",
      "Epoch 38/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 31693559824384.0000 - val_loss: 44229023760384.0000\n",
      "Epoch 39/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 30626157690880.0000 - val_loss: 44365942620160.0000\n",
      "Epoch 40/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 30491520532480.0000 - val_loss: 44395063672832.0000\n",
      "Epoch 41/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 30715854979072.0000 - val_loss: 45829695995904.0000\n",
      "Epoch 42/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 30835893862400.0000 - val_loss: 45073685282816.0000\n",
      "Epoch 43/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 30335953797120.0000 - val_loss: 46517758984192.0000\n",
      "Epoch 44/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 30912209223680.0000 - val_loss: 45062637486080.0000\n",
      "Epoch 45/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 28655237464064.0000 - val_loss: 45149560242176.0000\n",
      "Epoch 46/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 30223114436608.0000 - val_loss: 45964488343552.0000\n",
      "Epoch 47/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 29700833411072.0000 - val_loss: 45493786771456.0000\n",
      "Epoch 48/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 28029021585408.0000 - val_loss: 46429653434368.0000\n",
      "Epoch 49/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 28943293874176.0000 - val_loss: 46118347997184.0000\n",
      "Epoch 50/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 28777990062080.0000 - val_loss: 46420404994048.0000\n",
      "Epoch 51/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 28730059653120.0000 - val_loss: 46442253123584.0000\n",
      "Epoch 52/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 28425695789056.0000 - val_loss: 46449484103680.0000\n",
      "Epoch 53/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 26249877520384.0000 - val_loss: 46787435954176.0000\n",
      "Epoch 54/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 27322398801920.0000 - val_loss: 46999311220736.0000\n",
      "Epoch 55/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 27678467948544.0000 - val_loss: 46603226316800.0000\n",
      "Epoch 56/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 27474725437440.0000 - val_loss: 46302511497216.0000\n",
      "Epoch 57/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 27262359437312.0000 - val_loss: 46104431296512.0000\n",
      "Epoch 58/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 25398578511872.0000 - val_loss: 46757597675520.0000\n",
      "Epoch 59/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 26220750176256.0000 - val_loss: 47236041932800.0000\n",
      "Epoch 60/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 27151155855360.0000 - val_loss: 47545719980032.0000\n",
      "Epoch 61/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 25931114610688.0000 - val_loss: 48603120795648.0000\n",
      "Epoch 62/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 23648983318528.0000 - val_loss: 46917056724992.0000\n",
      "Epoch 63/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 26199973691392.0000 - val_loss: 46967971381248.0000\n",
      "Epoch 64/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 25160281227264.0000 - val_loss: 46846437228544.0000\n",
      "Epoch 65/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 25459219759104.0000 - val_loss: 47301389189120.0000\n",
      "Epoch 66/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 24926536859648.0000 - val_loss: 48066040168448.0000\n",
      "Epoch 67/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 25094407585792.0000 - val_loss: 48733794336768.0000\n",
      "Epoch 68/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 25439993069568.0000 - val_loss: 47649369620480.0000\n",
      "Epoch 69/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 25843252330496.0000 - val_loss: 48535609278464.0000\n",
      "Epoch 70/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 24043463901184.0000 - val_loss: 47708874211328.0000\n",
      "Epoch 71/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 24586347347968.0000 - val_loss: 48898898919424.0000\n",
      "Epoch 72/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 24629678702592.0000 - val_loss: 48121199460352.0000\n",
      "Epoch 73/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 24130323742720.0000 - val_loss: 49588069203968.0000\n",
      "Epoch 74/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 24387526852608.0000 - val_loss: 49119221514240.0000\n",
      "Epoch 75/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 24343811719168.0000 - val_loss: 50826722672640.0000\n",
      "Epoch 76/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 23327775129600.0000 - val_loss: 48961754759168.0000\n",
      "Epoch 77/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 23406485438464.0000 - val_loss: 49319663108096.0000\n",
      "Epoch 78/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 23327443779584.0000 - val_loss: 50057927720960.0000\n",
      "Epoch 79/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 22356107984896.0000 - val_loss: 51102296834048.0000\n",
      "Epoch 80/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 23542687072256.0000 - val_loss: 49790591172608.0000\n",
      "Epoch 81/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 23341784104960.0000 - val_loss: 50199263182848.0000\n",
      "Epoch 82/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 23804828975104.0000 - val_loss: 52159412436992.0000\n",
      "Epoch 83/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 22538727981056.0000 - val_loss: 50223770501120.0000\n",
      "Epoch 84/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 21750320463872.0000 - val_loss: 50758410043392.0000\n",
      "Epoch 85/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 21844761509888.0000 - val_loss: 50551882514432.0000\n",
      "Epoch 86/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 22912383844352.0000 - val_loss: 50589375397888.0000\n",
      "Epoch 87/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 21796470390784.0000 - val_loss: 51256676581376.0000\n",
      "Epoch 88/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 21696511737856.0000 - val_loss: 50062440792064.0000\n",
      "Epoch 89/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 21893136515072.0000 - val_loss: 51844642504704.0000\n",
      "Epoch 90/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 21515435245568.0000 - val_loss: 51339316953088.0000\n",
      "Epoch 91/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 22244227022848.0000 - val_loss: 51672667652096.0000\n",
      "Epoch 92/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 22100360298496.0000 - val_loss: 51059988889600.0000\n",
      "Epoch 93/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 22174544953344.0000 - val_loss: 50548514488320.0000\n",
      "Epoch 94/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 21721606258688.0000 - val_loss: 49698903687168.0000\n",
      "Epoch 95/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 21418249027584.0000 - val_loss: 54024942387200.0000\n",
      "Epoch 96/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 20303759540224.0000 - val_loss: 49878776414208.0000\n",
      "Epoch 97/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 20775249641472.0000 - val_loss: 53124249157632.0000\n",
      "Epoch 98/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 21875801456640.0000 - val_loss: 52948235190272.0000\n",
      "Epoch 99/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 21943919050752.0000 - val_loss: 51253547630592.0000\n",
      "Epoch 100/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 21181862248448.0000 - val_loss: 53260731809792.0000\n",
      "Epoch 101/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 22031013773312.0000 - val_loss: 51733422145536.0000\n",
      "Epoch 102/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 22097837424640.0000 - val_loss: 52569858637824.0000\n",
      "Epoch 103/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 20801757642752.0000 - val_loss: 52826415824896.0000\n",
      "Epoch 104/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 20657595219968.0000 - val_loss: 53595135279104.0000\n",
      "Epoch 105/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 20995700162560.0000 - val_loss: 54043682537472.0000\n",
      "Epoch 106/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 20573679779840.0000 - val_loss: 53986946187264.0000\n",
      "Epoch 107/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 19921360650240.0000 - val_loss: 51816762966016.0000\n",
      "Epoch 108/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 21119578931200.0000 - val_loss: 51931636563968.0000\n",
      "Epoch 109/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 19652719673344.0000 - val_loss: 54314726850560.0000\n",
      "Epoch 110/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 20008946106368.0000 - val_loss: 51708621225984.0000\n",
      "Epoch 111/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 20989043802112.0000 - val_loss: 52902991233024.0000\n",
      "Epoch 112/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 20635336048640.0000 - val_loss: 52750905769984.0000\n",
      "Epoch 113/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 19643309752320.0000 - val_loss: 52183374495744.0000\n",
      "Epoch 114/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 20582181634048.0000 - val_loss: 53452201787392.0000\n",
      "Epoch 115/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 19744914669568.0000 - val_loss: 53186404548608.0000\n",
      "Epoch 116/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 19893940387840.0000 - val_loss: 52281915473920.0000\n",
      "Epoch 117/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 20764380102656.0000 - val_loss: 53489786945536.0000\n",
      "Epoch 118/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 19573323595776.0000 - val_loss: 52700125331456.0000\n",
      "Epoch 119/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 19705733578752.0000 - val_loss: 52767594905600.0000\n",
      "Epoch 120/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 20010481221632.0000 - val_loss: 53581038223360.0000\n",
      "Epoch 121/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 19843791192064.0000 - val_loss: 53618266865664.0000\n",
      "Epoch 122/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 19720505917440.0000 - val_loss: 54586886848512.0000\n",
      "Epoch 123/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 20709778653184.0000 - val_loss: 55479560568832.0000\n",
      "Epoch 124/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 19043039838208.0000 - val_loss: 53032280653824.0000\n",
      "Epoch 125/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 19570895093760.0000 - val_loss: 54023675707392.0000\n",
      "Epoch 126/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 18207849054208.0000 - val_loss: 58800748888064.0000\n",
      "Epoch 127/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 18434813329408.0000 - val_loss: 54093322125312.0000\n",
      "Epoch 128/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 20102078529536.0000 - val_loss: 52247857725440.0000\n",
      "Epoch 129/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 20002851782656.0000 - val_loss: 53633886453760.0000\n",
      "Epoch 130/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 19256248893440.0000 - val_loss: 54862880440320.0000\n",
      "Epoch 131/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 18791423541248.0000 - val_loss: 52292292182016.0000\n",
      "Epoch 132/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 19866341867520.0000 - val_loss: 52532491583488.0000\n",
      "Epoch 133/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 19275062444032.0000 - val_loss: 51303153664000.0000\n",
      "Epoch 134/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 18331918663680.0000 - val_loss: 53570061729792.0000\n",
      "Epoch 135/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 18816025231360.0000 - val_loss: 54768055615488.0000\n",
      "Epoch 136/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 18582593339392.0000 - val_loss: 53705160261632.0000\n",
      "Epoch 137/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 19143529070592.0000 - val_loss: 52518499385344.0000\n",
      "Epoch 138/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 17752819499008.0000 - val_loss: 55384144347136.0000\n",
      "Epoch 139/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 18851238510592.0000 - val_loss: 52348298723328.0000\n",
      "Epoch 140/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 19136535068672.0000 - val_loss: 53514835329024.0000\n",
      "Epoch 141/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 18429847273472.0000 - val_loss: 53357204996096.0000\n",
      "Epoch 142/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 18000283435008.0000 - val_loss: 54561138016256.0000\n",
      "Epoch 143/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 17941745631232.0000 - val_loss: 54111605096448.0000\n",
      "Epoch 144/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 19473453023232.0000 - val_loss: 55466310762496.0000\n",
      "Epoch 145/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 18331897692160.0000 - val_loss: 56173529137152.0000\n",
      "Epoch 146/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 19157980545024.0000 - val_loss: 54043716091904.0000\n",
      "Epoch 147/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 18529220820992.0000 - val_loss: 52736397672448.0000\n",
      "Epoch 148/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 17858274787328.0000 - val_loss: 54717317120000.0000\n",
      "Epoch 149/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 17298083545088.0000 - val_loss: 55941990973440.0000\n",
      "Epoch 150/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 19157498200064.0000 - val_loss: 53876006846464.0000\n",
      "Epoch 151/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 18924865323008.0000 - val_loss: 55836537782272.0000\n",
      "Epoch 152/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 18230806577152.0000 - val_loss: 54044961800192.0000\n",
      "Epoch 153/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 18259134906368.0000 - val_loss: 54576912793600.0000\n",
      "Epoch 154/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 18089349480448.0000 - val_loss: 54616217616384.0000\n",
      "Epoch 155/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 18833830051840.0000 - val_loss: 53804124864512.0000\n",
      "Epoch 156/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18494944968704.0000 - val_loss: 54649541361664.0000\n",
      "Epoch 157/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 17160318484480.0000 - val_loss: 53710050820096.0000\n",
      "Epoch 158/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 17504543965184.0000 - val_loss: 55532538822656.0000\n",
      "Epoch 159/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 18160583442432.0000 - val_loss: 56353754185728.0000\n",
      "Epoch 160/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18251079745536.0000 - val_loss: 58710655238144.0000\n",
      "Epoch 161/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18595509698560.0000 - val_loss: 54662631784448.0000\n",
      "Epoch 162/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18555116453888.0000 - val_loss: 54586618413056.0000\n",
      "Epoch 163/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 18307707043840.0000 - val_loss: 53284186357760.0000\n",
      "Epoch 164/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16661151219712.0000 - val_loss: 56047133786112.0000\n",
      "Epoch 165/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 17917823418368.0000 - val_loss: 53058578939904.0000\n",
      "Epoch 166/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 18360043569152.0000 - val_loss: 54155880169472.0000\n",
      "Epoch 167/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 17055266897920.0000 - val_loss: 55972085104640.0000\n",
      "Epoch 168/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 17066215079936.0000 - val_loss: 53159074463744.0000\n",
      "Epoch 169/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16879296970752.0000 - val_loss: 54555698003968.0000\n",
      "Epoch 170/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 17293852540928.0000 - val_loss: 54253267714048.0000\n",
      "Epoch 171/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18108731359232.0000 - val_loss: 55589325504512.0000\n",
      "Epoch 172/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 17483617533952.0000 - val_loss: 54120908062720.0000\n",
      "Epoch 173/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 17601671462912.0000 - val_loss: 54940135325696.0000\n",
      "Epoch 174/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 18484366934016.0000 - val_loss: 55249301667840.0000\n",
      "Epoch 175/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16632622612480.0000 - val_loss: 54035629473792.0000\n",
      "Epoch 176/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 16871096057856.0000 - val_loss: 52659377668096.0000\n",
      "Epoch 177/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 17592603377664.0000 - val_loss: 56741559533568.0000\n",
      "Epoch 178/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 17598334894080.0000 - val_loss: 54212121591808.0000\n",
      "Epoch 179/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 17039793061888.0000 - val_loss: 54132308180992.0000\n",
      "Epoch 180/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 16515011182592.0000 - val_loss: 56540614623232.0000\n",
      "Epoch 181/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 16603354759168.0000 - val_loss: 53951504318464.0000\n",
      "Epoch 182/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 17222270451712.0000 - val_loss: 54332900769792.0000\n",
      "Epoch 183/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 17340861251584.0000 - val_loss: 54231251812352.0000\n",
      "Epoch 184/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16665025708032.0000 - val_loss: 56994383790080.0000\n",
      "Epoch 185/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 16648588230656.0000 - val_loss: 54427213889536.0000\n",
      "Epoch 186/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16960712605696.0000 - val_loss: 53824098140160.0000\n",
      "Epoch 187/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16860669018112.0000 - val_loss: 54423485153280.0000\n",
      "Epoch 188/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 17267186204672.0000 - val_loss: 57384609251328.0000\n",
      "Epoch 189/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16363797086208.0000 - val_loss: 54002943262720.0000\n",
      "Epoch 190/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 17366818750464.0000 - val_loss: 54374550208512.0000\n",
      "Epoch 191/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16850791432192.0000 - val_loss: 54305239334912.0000\n",
      "Epoch 192/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 17134656684032.0000 - val_loss: 53573471698944.0000\n",
      "Epoch 193/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 16338870337536.0000 - val_loss: 54095205367808.0000\n",
      "Epoch 194/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 17042613731328.0000 - val_loss: 54760623308800.0000\n",
      "Epoch 195/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16635270266880.0000 - val_loss: 53133350797312.0000\n",
      "Epoch 196/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16724643545088.0000 - val_loss: 58154989649920.0000\n",
      "Epoch 197/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15937471250432.0000 - val_loss: 54368405553152.0000\n",
      "Epoch 198/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 18305224015872.0000 - val_loss: 55868640985088.0000\n",
      "Epoch 199/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 15794233671680.0000 - val_loss: 57601018560512.0000\n",
      "Epoch 200/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 16726066462720.0000 - val_loss: 54861441794048.0000\n",
      "Epoch 201/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 17307249147904.0000 - val_loss: 56198732709888.0000\n",
      "Epoch 202/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 17386526736384.0000 - val_loss: 59316774109184.0000\n",
      "Epoch 203/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16169679454208.0000 - val_loss: 55753440231424.0000\n",
      "Epoch 204/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16662534291456.0000 - val_loss: 55317446524928.0000\n",
      "Epoch 205/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16414537678848.0000 - val_loss: 56337820024832.0000\n",
      "Epoch 206/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 16374230417408.0000 - val_loss: 56711465402368.0000\n",
      "Epoch 207/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 15451245510656.0000 - val_loss: 54487129522176.0000\n",
      "Epoch 208/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16862102421504.0000 - val_loss: 55389374644224.0000\n",
      "Epoch 209/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16516534763520.0000 - val_loss: 55815130054656.0000\n",
      "Epoch 210/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16361542647808.0000 - val_loss: 55056690839552.0000\n",
      "Epoch 211/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 16541341974528.0000 - val_loss: 54730550149120.0000\n",
      "Epoch 212/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 15810297856000.0000 - val_loss: 54461166780416.0000\n",
      "Epoch 213/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 16578166915072.0000 - val_loss: 54307382624256.0000\n",
      "Epoch 214/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16147488440320.0000 - val_loss: 53874220072960.0000\n",
      "Epoch 215/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 15880664645632.0000 - val_loss: 54873630441472.0000\n",
      "Epoch 216/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15900600172544.0000 - val_loss: 54399401459712.0000\n",
      "Epoch 217/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 16769445003264.0000 - val_loss: 56060278734848.0000\n",
      "Epoch 218/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15830102310912.0000 - val_loss: 55287042015232.0000\n",
      "Epoch 219/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15853123796992.0000 - val_loss: 54862150631424.0000\n",
      "Epoch 220/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 15892960247808.0000 - val_loss: 57471162908672.0000\n",
      "Epoch 221/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 15663860023296.0000 - val_loss: 57303101341696.0000\n",
      "Epoch 222/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16136216248320.0000 - val_loss: 55847837237248.0000\n",
      "Epoch 223/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15594414931968.0000 - val_loss: 54001991155712.0000\n",
      "Epoch 224/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16245364621312.0000 - val_loss: 55530009657344.0000\n",
      "Epoch 225/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 16084234141696.0000 - val_loss: 54349631848448.0000\n",
      "Epoch 226/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16990762696704.0000 - val_loss: 58212711661568.0000\n",
      "Epoch 227/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 15075003858944.0000 - val_loss: 56607778013184.0000\n",
      "Epoch 228/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 17136737058816.0000 - val_loss: 56059888664576.0000\n",
      "Epoch 229/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 16592793501696.0000 - val_loss: 55034792378368.0000\n",
      "Epoch 230/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 15994648002560.0000 - val_loss: 56821746237440.0000\n",
      "Epoch 231/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16355166257152.0000 - val_loss: 56156722561024.0000\n",
      "Epoch 232/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16704968065024.0000 - val_loss: 57262575976448.0000\n",
      "Epoch 233/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 15471749365760.0000 - val_loss: 55436992577536.0000\n",
      "Epoch 234/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 15721720446976.0000 - val_loss: 55601514151936.0000\n",
      "Epoch 235/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 16040974090240.0000 - val_loss: 58084403707904.0000\n",
      "Epoch 236/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 15189037547520.0000 - val_loss: 54643400900608.0000\n",
      "Epoch 237/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14884204969984.0000 - val_loss: 54770211487744.0000\n",
      "Epoch 238/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 15545709625344.0000 - val_loss: 54842387070976.0000\n",
      "Epoch 239/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 16593275846656.0000 - val_loss: 54946955264000.0000\n",
      "Epoch 240/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16108573687808.0000 - val_loss: 55099065892864.0000\n",
      "Epoch 241/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 16271977480192.0000 - val_loss: 54048405323776.0000\n",
      "Epoch 242/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15689055207424.0000 - val_loss: 55650742697984.0000\n",
      "Epoch 243/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 15881874702336.0000 - val_loss: 55588788633600.0000\n",
      "Epoch 244/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 15117846577152.0000 - val_loss: 55466952491008.0000\n",
      "Epoch 245/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15620183687168.0000 - val_loss: 56721426874368.0000\n",
      "Epoch 246/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15304877932544.0000 - val_loss: 57017058197504.0000\n",
      "Epoch 247/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16311238262784.0000 - val_loss: 56132592730112.0000\n",
      "Epoch 248/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14612065943552.0000 - val_loss: 55461558616064.0000\n",
      "Epoch 249/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14817740980224.0000 - val_loss: 57500384624640.0000\n",
      "Epoch 250/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 16219889467392.0000 - val_loss: 55812865130496.0000\n",
      "Epoch 251/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 15367454851072.0000 - val_loss: 56394300522496.0000\n",
      "Epoch 252/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 15915947130880.0000 - val_loss: 58911688228864.0000\n",
      "Epoch 253/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15630710341632.0000 - val_loss: 56100418224128.0000\n",
      "Epoch 254/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 16296922054656.0000 - val_loss: 56154101121024.0000\n",
      "Epoch 255/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 14796063768576.0000 - val_loss: 55487924011008.0000\n",
      "Epoch 256/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15204679155712.0000 - val_loss: 56023536631808.0000\n",
      "Epoch 257/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15693207568384.0000 - val_loss: 54991570075648.0000\n",
      "Epoch 258/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15438556692480.0000 - val_loss: 56572382281728.0000\n",
      "Epoch 259/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16688289415168.0000 - val_loss: 58542283292672.0000\n",
      "Epoch 260/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 16274412273664.0000 - val_loss: 55856678830080.0000\n",
      "Epoch 261/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 15682181791744.0000 - val_loss: 57735051739136.0000\n",
      "Epoch 262/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15249406164992.0000 - val_loss: 54241674657792.0000\n",
      "Epoch 263/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 15717732712448.0000 - val_loss: 58745346326528.0000\n",
      "Epoch 264/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15021306281984.0000 - val_loss: 56723482083328.0000\n",
      "Epoch 265/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15450035453952.0000 - val_loss: 56477867835392.0000\n",
      "Epoch 266/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 15181059981312.0000 - val_loss: 57412484595712.0000\n",
      "Epoch 267/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15136984137728.0000 - val_loss: 58842796785664.0000\n",
      "Epoch 268/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 14660108550144.0000 - val_loss: 54643816136704.0000\n",
      "Epoch 269/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 15547581333504.0000 - val_loss: 55779285532672.0000\n",
      "Epoch 270/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 15204388700160.0000 - val_loss: 56109750550528.0000\n",
      "Epoch 271/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14513243947008.0000 - val_loss: 56895339495424.0000\n",
      "Epoch 272/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 16131106537472.0000 - val_loss: 54622144167936.0000\n",
      "Epoch 273/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14033618993152.0000 - val_loss: 56303091187712.0000\n",
      "Epoch 274/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 15149718044672.0000 - val_loss: 56127848972288.0000\n",
      "Epoch 275/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - loss: 14471162494976.0000 - val_loss: 54747239284736.0000\n",
      "Epoch 276/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 15403769135104.0000 - val_loss: 55552465960960.0000\n",
      "Epoch 277/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 16292181442560.0000 - val_loss: 55192741478400.0000\n",
      "Epoch 278/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 14869386493952.0000 - val_loss: 56475573551104.0000\n",
      "Epoch 279/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 15175617871872.0000 - val_loss: 56499627884544.0000\n",
      "Epoch 280/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15606128574464.0000 - val_loss: 59381618049024.0000\n",
      "Epoch 281/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 16076456853504.0000 - val_loss: 54457073139712.0000\n",
      "Epoch 282/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14005380841472.0000 - val_loss: 55711430082560.0000\n",
      "Epoch 283/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 15154608603136.0000 - val_loss: 54915330211840.0000\n",
      "Epoch 284/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 15829915664384.0000 - val_loss: 56683464228864.0000\n",
      "Epoch 285/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14849023148032.0000 - val_loss: 59626888364032.0000\n",
      "Epoch 286/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 15190789718016.0000 - val_loss: 55932713172992.0000\n",
      "Epoch 287/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 14878159929344.0000 - val_loss: 55483121532928.0000\n",
      "Epoch 288/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14859487936512.0000 - val_loss: 57581775093760.0000\n",
      "Epoch 289/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15326408343552.0000 - val_loss: 54807112974336.0000\n",
      "Epoch 290/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 14589040263168.0000 - val_loss: 57208016470016.0000\n",
      "Epoch 291/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14828113494016.0000 - val_loss: 55389265592320.0000\n",
      "Epoch 292/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14999814668288.0000 - val_loss: 56516333797376.0000\n",
      "Epoch 293/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 14446269300736.0000 - val_loss: 56727479255040.0000\n",
      "Epoch 294/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14840797069312.0000 - val_loss: 56664535334912.0000\n",
      "Epoch 295/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 14605911851008.0000 - val_loss: 60300225150976.0000\n",
      "Epoch 296/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 14427077214208.0000 - val_loss: 54898955649024.0000\n",
      "Epoch 297/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 13999837020160.0000 - val_loss: 56584013086720.0000\n",
      "Epoch 298/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 15364167565312.0000 - val_loss: 54628293017600.0000\n",
      "Epoch 299/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15416077320192.0000 - val_loss: 57320709029888.0000\n",
      "Epoch 300/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14745363021824.0000 - val_loss: 56114464948224.0000\n",
      "Epoch 301/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14631233912832.0000 - val_loss: 57182179557376.0000\n",
      "Epoch 302/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15533074284544.0000 - val_loss: 57906951094272.0000\n",
      "Epoch 303/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 14436838408192.0000 - val_loss: 54795326980096.0000\n",
      "Epoch 304/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15122615500800.0000 - val_loss: 55177998499840.0000\n",
      "Epoch 305/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14481543397376.0000 - val_loss: 58229186887680.0000\n",
      "Epoch 306/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15760349986816.0000 - val_loss: 55773539336192.0000\n",
      "Epoch 307/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14776806670336.0000 - val_loss: 60800647561216.0000\n",
      "Epoch 308/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14983723220992.0000 - val_loss: 55463508967424.0000\n",
      "Epoch 309/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14690838118400.0000 - val_loss: 56244521926656.0000\n",
      "Epoch 310/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13863334445056.0000 - val_loss: 57340959129600.0000\n",
      "Epoch 311/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 14377544581120.0000 - val_loss: 55752848834560.0000\n",
      "Epoch 312/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14422016786432.0000 - val_loss: 54885584207872.0000\n",
      "Epoch 313/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15454584176640.0000 - val_loss: 55663937978368.0000\n",
      "Epoch 314/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14260387184640.0000 - val_loss: 56755488817152.0000\n",
      "Epoch 315/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14508434128896.0000 - val_loss: 56913110761472.0000\n",
      "Epoch 316/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14471204438016.0000 - val_loss: 57309455712256.0000\n",
      "Epoch 317/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13466799702016.0000 - val_loss: 56923680407552.0000\n",
      "Epoch 318/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15471673868288.0000 - val_loss: 55479979999232.0000\n",
      "Epoch 319/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 14586373734400.0000 - val_loss: 57722053591040.0000\n",
      "Epoch 320/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13878062743552.0000 - val_loss: 57194489839616.0000\n",
      "Epoch 321/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13906242174976.0000 - val_loss: 57708157861888.0000\n",
      "Epoch 322/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14355268632576.0000 - val_loss: 56171989827584.0000\n",
      "Epoch 323/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14052072882176.0000 - val_loss: 56149936177152.0000\n",
      "Epoch 324/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 14681638961152.0000 - val_loss: 55591678509056.0000\n",
      "Epoch 325/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14479594094592.0000 - val_loss: 58118935412736.0000\n",
      "Epoch 326/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14606575599616.0000 - val_loss: 57743461318656.0000\n",
      "Epoch 327/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14222142472192.0000 - val_loss: 58516509294592.0000\n",
      "Epoch 328/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15647604998144.0000 - val_loss: 57391806676992.0000\n",
      "Epoch 329/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14132389609472.0000 - val_loss: 57539634921472.0000\n",
      "Epoch 330/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13457415995392.0000 - val_loss: 58943464275968.0000\n",
      "Epoch 331/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14067732316160.0000 - val_loss: 57904568729600.0000\n",
      "Epoch 332/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15132163833856.0000 - val_loss: 57280523403264.0000\n",
      "Epoch 333/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14463566610432.0000 - val_loss: 57308323250176.0000\n",
      "Epoch 334/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13706273488896.0000 - val_loss: 56190843224064.0000\n",
      "Epoch 335/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 14249284861952.0000 - val_loss: 54961568219136.0000\n",
      "Epoch 336/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 14478739505152.0000 - val_loss: 58568208285696.0000\n",
      "Epoch 337/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 14050168668160.0000 - val_loss: 57743998189568.0000\n",
      "Epoch 338/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14130388926464.0000 - val_loss: 56088300879872.0000\n",
      "Epoch 339/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13813124431872.0000 - val_loss: 55572380516352.0000\n",
      "Epoch 340/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14711414325248.0000 - val_loss: 57029364285440.0000\n",
      "Epoch 341/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13932288802816.0000 - val_loss: 56120064344064.0000\n",
      "Epoch 342/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14502835781632.0000 - val_loss: 57938379014144.0000\n",
      "Epoch 343/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 14254971289600.0000 - val_loss: 56986553024512.0000\n",
      "Epoch 344/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14440264105984.0000 - val_loss: 58793979281408.0000\n",
      "Epoch 345/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13790145937408.0000 - val_loss: 58608037396480.0000\n",
      "Epoch 346/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 14674799099904.0000 - val_loss: 57233782079488.0000\n",
      "Epoch 347/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14770715492352.0000 - val_loss: 56999555366912.0000\n",
      "Epoch 348/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14192324116480.0000 - val_loss: 57590855761920.0000\n",
      "Epoch 349/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 14231800905728.0000 - val_loss: 58033891704832.0000\n",
      "Epoch 350/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 14569764290560.0000 - val_loss: 55718052888576.0000\n",
      "Epoch 351/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 14118999293952.0000 - val_loss: 57090731147264.0000\n",
      "Epoch 352/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14371305553920.0000 - val_loss: 56504187092992.0000\n",
      "Epoch 353/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 13797548883968.0000 - val_loss: 56238838644736.0000\n",
      "Epoch 354/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 13605242142720.0000 - val_loss: 55138001616896.0000\n",
      "Epoch 355/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14537830957056.0000 - val_loss: 58984983691264.0000\n",
      "Epoch 356/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 14020065099776.0000 - val_loss: 57271983800320.0000\n",
      "Epoch 357/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13564208218112.0000 - val_loss: 57014801661952.0000\n",
      "Epoch 358/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15724803260416.0000 - val_loss: 55442336120832.0000\n",
      "Epoch 359/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15094692970496.0000 - val_loss: 56171234852864.0000\n",
      "Epoch 360/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13331908788224.0000 - val_loss: 57457694998528.0000\n",
      "Epoch 361/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14073069568000.0000 - val_loss: 56494837989376.0000\n",
      "Epoch 362/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 13222473105408.0000 - val_loss: 59023067971584.0000\n",
      "Epoch 363/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13960683192320.0000 - val_loss: 55725543915520.0000\n",
      "Epoch 364/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13312436731904.0000 - val_loss: 58779014004736.0000\n",
      "Epoch 365/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 13803968266240.0000 - val_loss: 56802985115648.0000\n",
      "Epoch 366/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14011811758080.0000 - val_loss: 56344090509312.0000\n",
      "Epoch 367/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 15093122203648.0000 - val_loss: 57417979133952.0000\n",
      "Epoch 368/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13988640325632.0000 - val_loss: 56505256640512.0000\n",
      "Epoch 369/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14312032698368.0000 - val_loss: 56728045486080.0000\n",
      "Epoch 370/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13274161610752.0000 - val_loss: 57197438435328.0000\n",
      "Epoch 371/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14255993651200.0000 - val_loss: 59705879691264.0000\n",
      "Epoch 372/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 15197779525632.0000 - val_loss: 58447487827968.0000\n",
      "Epoch 373/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 14398521344000.0000 - val_loss: 56298867523584.0000\n",
      "Epoch 374/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13723588624384.0000 - val_loss: 57359070134272.0000\n",
      "Epoch 375/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13896046870528.0000 - val_loss: 57636582064128.0000\n",
      "Epoch 376/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 13399649943552.0000 - val_loss: 55624326971392.0000\n",
      "Epoch 377/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 14099764215808.0000 - val_loss: 56504904318976.0000\n",
      "Epoch 378/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14121994027008.0000 - val_loss: 57330787942400.0000\n",
      "Epoch 379/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14403004006400.0000 - val_loss: 57402376323072.0000\n",
      "Epoch 380/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13946973061120.0000 - val_loss: 59523351969792.0000\n",
      "Epoch 381/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14867732889600.0000 - val_loss: 55998857347072.0000\n",
      "Epoch 382/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15225197690880.0000 - val_loss: 56928503857152.0000\n",
      "Epoch 383/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14072891310080.0000 - val_loss: 58167706779648.0000\n",
      "Epoch 384/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 14370798043136.0000 - val_loss: 58863000748032.0000\n",
      "Epoch 385/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14431055511552.0000 - val_loss: 55842598551552.0000\n",
      "Epoch 386/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 15047293140992.0000 - val_loss: 58091550801920.0000\n",
      "Epoch 387/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 14905555025920.0000 - val_loss: 56399207858176.0000\n",
      "Epoch 388/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 13722715160576.0000 - val_loss: 56037604327424.0000\n",
      "Epoch 389/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13490390564864.0000 - val_loss: 54575973269504.0000\n",
      "Epoch 390/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 13454148632576.0000 - val_loss: 55185808293888.0000\n",
      "Epoch 391/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13871423160320.0000 - val_loss: 56661347663872.0000\n",
      "Epoch 392/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14230568828928.0000 - val_loss: 57173409267712.0000\n",
      "Epoch 393/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13464759173120.0000 - val_loss: 57284398940160.0000\n",
      "Epoch 394/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13836608339968.0000 - val_loss: 56710974668800.0000\n",
      "Epoch 395/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14163522879488.0000 - val_loss: 57472555417600.0000\n",
      "Epoch 396/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 15220666793984.0000 - val_loss: 57893520932864.0000\n",
      "Epoch 397/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13677043384320.0000 - val_loss: 55601086332928.0000\n",
      "Epoch 398/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13603697590272.0000 - val_loss: 56317662199808.0000\n",
      "Epoch 399/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 14020761354240.0000 - val_loss: 57312475611136.0000\n",
      "Epoch 400/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 14175975768064.0000 - val_loss: 57331446448128.0000\n",
      "Epoch 401/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13495153197056.0000 - val_loss: 55895971069952.0000\n",
      "Epoch 402/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 14538008166400.0000 - val_loss: 57017003671552.0000\n",
      "Epoch 403/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13834527965184.0000 - val_loss: 56610151989248.0000\n",
      "Epoch 404/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - loss: 13363168935936.0000 - val_loss: 58049993637888.0000\n",
      "Epoch 405/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 13713697406976.0000 - val_loss: 57250555101184.0000\n",
      "Epoch 406/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13278661050368.0000 - val_loss: 56873613000704.0000\n",
      "Epoch 407/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 14726662717440.0000 - val_loss: 57606517293056.0000\n",
      "Epoch 408/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 14471554662400.0000 - val_loss: 58214519406592.0000\n",
      "Epoch 409/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13347501113344.0000 - val_loss: 57589039628288.0000\n",
      "Epoch 410/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 13951017418752.0000 - val_loss: 58447689154560.0000\n",
      "Epoch 411/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 13932734447616.0000 - val_loss: 57917638180864.0000\n",
      "Epoch 412/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13268728938496.0000 - val_loss: 57447083409408.0000\n",
      "Epoch 413/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 13556360675328.0000 - val_loss: 57109173501952.0000\n",
      "Epoch 414/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 13633466662912.0000 - val_loss: 56591973875712.0000\n",
      "Epoch 415/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 15080765784064.0000 - val_loss: 56894525800448.0000\n",
      "Epoch 416/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - loss: 13248472547328.0000 - val_loss: 56277325578240.0000\n",
      "Epoch 417/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 14003585679360.0000 - val_loss: 55911808761856.0000\n",
      "Epoch 418/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 13088675856384.0000 - val_loss: 58206499897344.0000\n",
      "Epoch 419/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 13686393536512.0000 - val_loss: 55492810375168.0000\n",
      "Epoch 420/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 12714401333248.0000 - val_loss: 56668255682560.0000\n",
      "Epoch 421/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 13517711212544.0000 - val_loss: 56528895737856.0000\n",
      "Epoch 422/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - loss: 13319955021824.0000 - val_loss: 56538056097792.0000\n",
      "Epoch 423/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 13943514857472.0000 - val_loss: 56363308810240.0000\n",
      "Epoch 424/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13450232201216.0000 - val_loss: 58875378139136.0000\n",
      "Epoch 425/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 13522263080960.0000 - val_loss: 57207387324416.0000\n",
      "Epoch 426/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 13310381522944.0000 - val_loss: 56746995351552.0000\n",
      "Epoch 427/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 13262484668416.0000 - val_loss: 57491358482432.0000\n",
      "Epoch 428/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 14512603267072.0000 - val_loss: 56443227078656.0000\n",
      "Epoch 429/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 12803985375232.0000 - val_loss: 56239564259328.0000\n",
      "Epoch 430/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14379199234048.0000 - val_loss: 57547235000320.0000\n",
      "Epoch 431/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 14096196960256.0000 - val_loss: 57477622136832.0000\n",
      "Epoch 432/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 12939384848384.0000 - val_loss: 58328407343104.0000\n",
      "Epoch 433/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13232768024576.0000 - val_loss: 57590268559360.0000\n",
      "Epoch 434/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13950327455744.0000 - val_loss: 55645118136320.0000\n",
      "Epoch 435/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13504093356032.0000 - val_loss: 55651346677760.0000\n",
      "Epoch 436/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13332767571968.0000 - val_loss: 55890501697536.0000\n",
      "Epoch 437/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 12561251565568.0000 - val_loss: 57351902068736.0000\n",
      "Epoch 438/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 14652539928576.0000 - val_loss: 56598751870976.0000\n",
      "Epoch 439/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13072326459392.0000 - val_loss: 56207633022976.0000\n",
      "Epoch 440/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - loss: 14009618137088.0000 - val_loss: 55592399929344.0000\n",
      "Epoch 441/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 13369172033536.0000 - val_loss: 57094434717696.0000\n",
      "Epoch 442/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13120992968704.0000 - val_loss: 55500146212864.0000\n",
      "Epoch 443/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 13133179518976.0000 - val_loss: 58471412137984.0000\n",
      "Epoch 444/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 14131870564352.0000 - val_loss: 56717253541888.0000\n",
      "Epoch 445/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13053700603904.0000 - val_loss: 57004055855104.0000\n",
      "Epoch 446/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13940278951936.0000 - val_loss: 56980446117888.0000\n",
      "Epoch 447/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13822653890560.0000 - val_loss: 57305093636096.0000\n",
      "Epoch 448/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 14253520060416.0000 - val_loss: 57878878617600.0000\n",
      "Epoch 449/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13724400222208.0000 - val_loss: 57444864622592.0000\n",
      "Epoch 450/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 13349109628928.0000 - val_loss: 57421900808192.0000\n",
      "Epoch 451/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13439241027584.0000 - val_loss: 58024303525888.0000\n",
      "Epoch 452/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 14072502288384.0000 - val_loss: 57278782767104.0000\n",
      "Epoch 453/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13625762775040.0000 - val_loss: 56541877108736.0000\n",
      "Epoch 454/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13223220740096.0000 - val_loss: 57324307742720.0000\n",
      "Epoch 455/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13792573390848.0000 - val_loss: 56539826094080.0000\n",
      "Epoch 456/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 13768297807872.0000 - val_loss: 57117515972608.0000\n",
      "Epoch 457/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 14141156753408.0000 - val_loss: 57539089661952.0000\n",
      "Epoch 458/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13967384641536.0000 - val_loss: 59122363924480.0000\n",
      "Epoch 459/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 12458314956800.0000 - val_loss: 57193344794624.0000\n",
      "Epoch 460/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13133829636096.0000 - val_loss: 57994968563712.0000\n",
      "Epoch 461/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 14081974075392.0000 - val_loss: 57301117435904.0000\n",
      "Epoch 462/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 13813663399936.0000 - val_loss: 56804935467008.0000\n",
      "Epoch 463/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 12913210294272.0000 - val_loss: 56280685215744.0000\n",
      "Epoch 464/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13553110089728.0000 - val_loss: 60169056681984.0000\n",
      "Epoch 465/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13725861937152.0000 - val_loss: 57097161015296.0000\n",
      "Epoch 466/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13486547533824.0000 - val_loss: 57229642301440.0000\n",
      "Epoch 467/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - loss: 13853123411968.0000 - val_loss: 57527853121536.0000\n",
      "Epoch 468/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 12228583489536.0000 - val_loss: 57008711532544.0000\n",
      "Epoch 469/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 13113799737344.0000 - val_loss: 56642599124992.0000\n",
      "Epoch 470/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 12719653650432.0000 - val_loss: 56586009575424.0000\n",
      "Epoch 471/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 12813304070144.0000 - val_loss: 57255340802048.0000\n",
      "Epoch 472/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13061214699520.0000 - val_loss: 56234120052736.0000\n",
      "Epoch 473/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 12507539308544.0000 - val_loss: 58343905296384.0000\n",
      "Epoch 474/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 13871187230720.0000 - val_loss: 57166257979392.0000\n",
      "Epoch 475/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 13343599362048.0000 - val_loss: 56846568128512.0000\n",
      "Epoch 476/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - loss: 12958618877952.0000 - val_loss: 58450767773696.0000\n",
      "Epoch 477/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 13107652984832.0000 - val_loss: 56256437944320.0000\n",
      "Epoch 478/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 13585824612352.0000 - val_loss: 56196451008512.0000\n",
      "Epoch 479/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13352109604864.0000 - val_loss: 56157838245888.0000\n",
      "Epoch 480/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 12987159019520.0000 - val_loss: 57186554216448.0000\n",
      "Epoch 481/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 13229650608128.0000 - val_loss: 55253185593344.0000\n",
      "Epoch 482/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 13799242334208.0000 - val_loss: 56363891818496.0000\n",
      "Epoch 483/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 13989350211584.0000 - val_loss: 56701373906944.0000\n",
      "Epoch 484/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13672987492352.0000 - val_loss: 56248896585728.0000\n",
      "Epoch 485/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 12768760561664.0000 - val_loss: 57037383794688.0000\n",
      "Epoch 486/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13815388307456.0000 - val_loss: 59484361719808.0000\n",
      "Epoch 487/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 12739154018304.0000 - val_loss: 57592562843648.0000\n",
      "Epoch 488/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13228451037184.0000 - val_loss: 58052984176640.0000\n",
      "Epoch 489/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 12349215866880.0000 - val_loss: 56082898616320.0000\n",
      "Epoch 490/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13275276247040.0000 - val_loss: 59234100183040.0000\n",
      "Epoch 491/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 12650994991104.0000 - val_loss: 57787673477120.0000\n",
      "Epoch 492/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 13211401191424.0000 - val_loss: 57745411670016.0000\n",
      "Epoch 493/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 13511644151808.0000 - val_loss: 58827177197568.0000\n",
      "Epoch 494/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 13449686941696.0000 - val_loss: 56120110481408.0000\n",
      "Epoch 495/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - loss: 14471992967168.0000 - val_loss: 56864431669248.0000\n",
      "Epoch 496/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 12531206717440.0000 - val_loss: 56302998913024.0000\n",
      "Epoch 497/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 13498773929984.0000 - val_loss: 55985116807168.0000\n",
      "Epoch 498/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 12824253300736.0000 - val_loss: 55877885231104.0000\n",
      "Epoch 499/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 13064646688768.0000 - val_loss: 57951612043264.0000\n",
      "Epoch 500/500\n",
      "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 13178125680640.0000 - val_loss: 58153341288448.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x393bf8410>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_optimized_1d_cnn(input_dim=X_train_scaled_cnn.shape[1])\n",
    "model.fit(X_train_scaled_cnn, y_aug, epochs=500, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cb0d5f82-2dec-4b1e-a53a-8506041f7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "MAE: 4011558.5\n",
      "R²: 0.5960239171981812\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled_cnn)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a20867-1f99-4ac0-be00-519da9fc0cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
